---
title: "Начало работы"
description: "Начальная настройка Membria CE (Персональная версия): подключение источников, первый запуск индексации и начало накопления устойчивого интеллекта."
---

> Membria CE — это персональная ИИ-система с приоритетом локальной обработки (local-first) и слоем персистентности. Вы можете использовать её как обычный чат с первого дня, но она будет постепенно выстраивать приватную память рассуждений и знаний на основе вашей реальной работы.

## 1) Выберите способ запуска Membria CE

Membria CE поддерживает два режима:

- **Облако (managed)**: самый быстрый способ начать. Ваш персональный воркспейс хостится в облаке, и вы просто подключаете свои источники.
- **Self-hosted (локально)**: установка и запуск CE локально (когда ваш пакет будет доступен). Локальные данные остаются локальными по умолчанию.

В обоих режимах Membria может использовать **децентрализованный бэкенд знаний** в качестве общего слоя кэширования (под вашим контролем), чтобы ответы, цитаты и извлеченные знания сохранялись за пределами одного устройства.

## 2) Создайте ваш персональный воркспейс

Создайте воркспейс, который будет содержать:

- ваши подключенные источники;
- ваш персональный граф рассуждений (память);
- события вашего Decision Black Box (DBB);
- ваш Decision Surface (DS).

Если вы планируете позже перейти на self-hosted версию, старайтесь сохранять название воркспейса и структуру источников чистыми (это упростит экспорт и миграцию).

## 3) Подключите первый источник (минимум один)

Membria работает лучше всего, когда у неё есть хотя бы один реальный источник для опоры (grounding). Начните с одного:

- папка с документами (PDF, DOCX, MD);
- коннектор Google Drive или Dropbox;
- экспорт чатов (логи Claude, Codex, ChatGPT);
- репозиторий заметок (Obsidian или Markdown).

**Совет:** Начните с малого (последние 30–90 дней). Вы сможете расширить охват позже, когда начнете доверять сигналам.

## 4) Запустите первую индексацию (ingestion)

Запустите процесс и дождитесь трех результатов:

1. **Индексация** (текст с поиском + метаданные);
2. **Семена графовой памяти** (сущности, связи, таймлайны, происхождение);
3. **Первые кандидаты DBB** (потенциальные моменты принятия решений, обнаруженные в ваших материалах).

На этом этапе Membria не пытается быть «умной». Она пытается быть *обоснованной*.

## 5) Откройте Decision Surface (DS) и изучите первые сигналы

Decision Surface — это главный экран, потому что он показывает *то, что важно во времени*, а не то, что было сказано последним.

Ищите:

- **открытые** решения (не завершенные);
- решения с высоким **VoI (ценность информации)** — где новые данные могут существенно изменить исход;
- **скрытые разногласия** (сигналы POMDP) — где поверхностное согласие скрывает низкий консенсус;
- повторяющиеся темы с **дрифтом** (когда одно и то же решение принимается снова и снова).

Не стремитесь к совершенству. Сейчас вы занимаетесь калибровкой.

## 6) Подтвердите или отклоните захваты DBB (первая петля калибровки)

DBB (Decision Black Box) будет периодически подсвечивать: «Мне кажется, здесь было принято решение».

Ваша задача в первую неделю проста:

- **Подтвердить**, если это действительно было решение (и, по желанию, пометить намерение или исход);
- **Отклонить**, если это был просто шум обсуждения.

Это обучает пороги Membria и снижает количество ложных срабатываний без изменения вашего рабочего процесса.

## 7) Используйте режим чата как обычно

Вы можете общаться в чате точно так же, как в других инструментах.

Разница в том, что происходит «под капотом»:

- чаты рассматриваются как **входные потоки данных**;
- Membria извлекает **решения, рассуждения, предположения и исходы**;
- DS становится чистой панелью управления над хаосом разговоров.

Вам не нужно учиться писать лучшие промпты или внедрять новую дисциплину, чтобы Membria работала.

## 8) Что происходит, когда локального ИИ недостаточно

Когда локальная модель не уверена, Membria эскалирует запрос в строго определенном порядке:

1. **Локальная память + GraphRAG** (ваши источники в первую очередь);
2. **Поиск в кэше знаний** (если верифицированный ответ уже существует);
3. **Эскалация в Совет (Council)** (более мощные модели синтезируют лучший ответ);
4. **Кэширование + обучение** (результат сохраняется с указанием происхождения для повторного использования).

Эскалации видимы, логируются и могут быть ограничены бюджетом.

## 9) Роль LoRA (без вашего участия в управлении)

Membria может применять **LoRA-патчи** для специализации вашей локальной модели в доменах, с которыми вы часто работаете.

В версии CE LoRA предназначена для:

- закрытия повторяющихся пробелов в знаниях, обнаруженных DBB и исходами;
- постепенного снижения частоты эскалаций;
- повышения согласованности в ваших доменах (без переобучения базовой модели).

Вам не нужно выбирать LoRA вручную. Membria подключает их, когда они полезны, и откатывает, если они снижают качество.

## 10) Следующие шаги

- Прочитайте раздел **Концепции**, чтобы понять принципы DBB, DS, графовой памяти, Совета и кэшей.
- Откройте **Рантайм клиента**, чтобы увидеть весь стек и понять, где живут данные.
- Добавьте второй источник (другого типа: документы + логи чатов — отличное сочетание).
- Через 7–14 дней снова проверьте DS и отметьте исходы (outcomes) для нескольких ключевых решений (именно тогда начинается эффект накопления).
